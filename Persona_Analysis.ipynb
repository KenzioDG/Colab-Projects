{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KenzioDG/Colab-Projects/blob/main/Persona_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada kasus ini, diberikan suatu dataset yang berisi abstrak dari berbagai sumber studi. Tujuan kasus ini adalah untuk membuat suatu metode pengelompokkan dokumen ini ke kategori masing-masing dengan memberikan coherence score dan persona analysis, karena pada dataset ini tidak terdapat label sama sekali\n",
        "\n",
        "Untuk mengelompokkan topik dari abstrak-abstrak yang diberikan tanpa menggunakan label, kita dapat menggunakan metode pengelompokan topik unsupervised, dalam kasus ini digunakan **Latent Dirichlet Allocation (LDA).** LDA adalah model generatif probabilistik yang digunakan untuk menganalisis topik dalam koleksi dokumen. Model ini merupakan salah satu metode yang populer dalam topic modeling yang bertujuan untuk mengidentifikasi topik-topik yang ada dalam dokumen secara otomatis."
      ],
      "metadata": {
        "id": "BRM8hc-j1gKK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3QOdxmBh-Rj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed988671-9921-43b4-b701-ca4f88d51122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13qGCLFSbXb8v231vsjjZh3GwlIyTAvhH\n",
            "To: /content/data_3A.csv\n",
            "\r  0% 0.00/96.2k [00:00<?, ?B/s]\r100% 96.2k/96.2k [00:00<00:00, 105MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 13qGCLFSbXb8v231vsjjZh3GwlIyTAvhH"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- `stopwords`: Pada kasus ini daftar stopwords akan digunakan dalam tahap preprocessing untuk menghapus stopwords dari teks abstrak.\n",
        "\n",
        "- `wordnet`: Dalam kasus ini, WordNet digunakan untuk melakukan lemmatisasi, yaitu mengubah kata-kata ke bentuk dasar mereka, seperti mengubah \"running\" menjadi \"run\".\n",
        "\n",
        "- `punkt`: Dalam kasus ini, pemodelan tokenisasi diperlukan untuk memisahkan teks abstrak menjadi kata-kata sebelum dilakukan tahap preprocessing."
      ],
      "metadata": {
        "id": "6Gf4TkOP2iYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim import corpora, models\n",
        "from gensim.models import CoherenceModel\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "# Download stopwords and lemmatizer data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXxojicB0Z7r",
        "outputId": "8bcede1a-6330-4551-8530-e73783dbb34f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "data = pd.read_csv('data_3A.csv')"
      ],
      "metadata": {
        "id": "2XgHTt9_0BTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "\n",
        "Stopwords dihapus dan kata-kata dibiarkan dalam bentuk dasar mereka dengan melakukan lemmatisasi menggunakan lemmatizer. Kata-kata yang ada dalam stop_words akan dihapus dari hasil lemmatisasi.\n",
        "\n",
        "Setelah proses preprocessing, dilakukan pembuatan kamus (dictionary) dan korpus.\n",
        "Kamus (dictionary) dibuat dengan menggunakan `corpora.Dictionary()` pada kolom `processed_abstract` dari dataframe data. Ini akan membuat mapping antara kata-kata unik dalam teks ke indeks numerik."
      ],
      "metadata": {
        "id": "1frTpGdJ5R25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Cleaning\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove URLs or links\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove non-alphabetic characters (keeping whitespace)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)  # Remove extra whitespace\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Join tokens back into string\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "data['processed_abstract'] = data['ABSTRACT'].apply(preprocess_text)\n",
        "\n",
        "# Create dictionary and corpus\n",
        "texts = [text.split() for text in data['processed_abstract']]\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n"
      ],
      "metadata": {
        "id": "4IJ7NKBiq7Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x57ksquC_11j",
        "outputId": "c9869723-9a67-4838-ac31-56f757db29fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 3 columns):\n",
            " #   Column              Non-Null Count  Dtype \n",
            "---  ------              --------------  ----- \n",
            " 0   Unnamed: 0          100 non-null    int64 \n",
            " 1   ABSTRACT            100 non-null    object\n",
            " 2   processed_abstract  100 non-null    object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 2.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasil text preprocessing kira-kira seperti di bawah ini"
      ],
      "metadata": {
        "id": "MNFujslEw6Sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['processed_abstract']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj2m29SQ8lgx",
        "outputId": "09a64d17-f817-4f37-c44a-d36b5daff214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     predictive model allow subjectspecific inferen...\n",
              "1     rotation invariance translation invariance gre...\n",
              "2     introduce develop notion spherical polyharmoni...\n",
              "3     stochastic landaulifshitzgilbert llg equation ...\n",
              "4     fouriertransform infrared ftir spectrum sample...\n",
              "                            ...                        \n",
              "95    paper presented novel convolutional neural net...\n",
              "96    variety representation learning approach inves...\n",
              "97    motivated perelmans pseudo locality theorem ri...\n",
              "98    bound exponential sum appears study irregulari...\n",
              "99    investigate effect dimensional crossover groun...\n",
              "Name: processed_abstract, Length: 100, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA Model\n",
        "Pada kode di bawah, dilakukan iterasi untuk melatih model LDA dengan jumlah topik yang berbeda dan juga analisis terkait hasilnya. Berikut adalah penjelasan langkah-langkah yang dilakukan dalam kode tersebut:\n",
        "\n",
        "- `num_topics_list` didefinisikan sebagai daftar yang berisi jumlah topik yang akan diuji, dalam contoh kode ini adalah [2, 3, 5, 7, 8]. Maka berarti model LDA akan dilatih dengan mengelompokkan ke topik sebanyak 2, 3, 5, 7, dan 8 buah secara terpisah\n",
        "\n",
        "- Dalam setiap iterasi, model LDA (lda_model) dilatih dengan menggunakan fungsi models.LdaModel(). Parameternya adalah korpus BoW yang telah dibuat sebelumnya, num_topics, id2word (kamus yang digunakan), dan passes (jumlah iterasi pada data).\n",
        "\n",
        "- Setelah melatih model LDA, coherence score dihitung menggunakan CoherenceModel dengan menggunakan model LDA, teks abstrak yang telah diproses\n",
        "\n",
        "- Setelah mendapatkan model LDA, persona analysis dilakukan dengan mencetak topik-topik yang dihasilkan oleh model menggunakan print_topics."
      ],
      "metadata": {
        "id": "-veYSPeu5geS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Berikut ini adalah pengelompokan dan output Coherence Score"
      ],
      "metadata": {
        "id": "w8HZC1RiLReF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define number of topics\n",
        "num_topics_list = [2, 4, 6, 8, 10]\n",
        "\n",
        "for num_topics in num_topics_list:\n",
        "    print(f\"Number of Topics: {num_topics}\")\n",
        "\n",
        "    # Train LDA model\n",
        "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=50, random_state=69)\n",
        "\n",
        "    # Evaluate coherence score\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model_lda.get_coherence()\n",
        "    print(f\"Coherence Score: {coherence_score}\\n\")\n",
        "\n",
        "    # Perform persona analysis\n",
        "    topics = lda_model.print_topics(num_topics=num_topics)\n",
        "    for topic in topics:\n",
        "        topic_num = topic[0] + 1  # Add 1 to start topic number from 1\n",
        "        print(f\"Topic {topic_num}: {topic[1]}\\n\")\n",
        "\n",
        "    # Count data in each topic\n",
        "    topic_counts = [0] * num_topics\n",
        "    total_docs = 0\n",
        "\n",
        "    for doc in corpus:\n",
        "        topic_distribution = lda_model[doc]\n",
        "        sorted_topics = sorted(topic_distribution, key=lambda x: x[1], reverse=True)\n",
        "        top_topic = sorted_topics[0][0]\n",
        "        topic_counts[top_topic] += 1\n",
        "        total_docs += 1\n",
        "\n",
        "    # Print percentage of data in each topic\n",
        "    for i, count in enumerate(topic_counts):\n",
        "        percentage = (count / total_docs) * 100\n",
        "        print(f\"Topic {i+1}: {percentage:.2f}%\")\n",
        "\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfrrC3LZ5kJI",
        "outputId": "0d17e8e3-6e8f-465b-8cda-543c52223255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Topics: 2\n",
            "Coherence Score: 0.290093230451281\n",
            "\n",
            "Topic 1: 0.006*\"model\" + 0.005*\"system\" + 0.004*\"method\" + 0.004*\"approach\" + 0.004*\"result\" + 0.003*\"used\" + 0.003*\"application\" + 0.003*\"performance\" + 0.003*\"sample\" + 0.003*\"data\"\n",
            "\n",
            "Topic 2: 0.009*\"model\" + 0.006*\"method\" + 0.005*\"state\" + 0.004*\"network\" + 0.004*\"result\" + 0.004*\"function\" + 0.004*\"also\" + 0.003*\"show\" + 0.003*\"based\" + 0.003*\"equation\"\n",
            "\n",
            "Topic 1: 43.00%\n",
            "Topic 2: 57.00%\n",
            "\n",
            "Number of Topics: 4\n",
            "Coherence Score: 0.34938396677322825\n",
            "\n",
            "Topic 1: 0.007*\"system\" + 0.007*\"model\" + 0.006*\"approach\" + 0.005*\"framework\" + 0.005*\"used\" + 0.004*\"show\" + 0.004*\"result\" + 0.003*\"well\" + 0.003*\"two\" + 0.003*\"study\"\n",
            "\n",
            "Topic 2: 0.009*\"model\" + 0.006*\"function\" + 0.006*\"network\" + 0.005*\"formation\" + 0.005*\"using\" + 0.004*\"x\" + 0.004*\"state\" + 0.004*\"domain\" + 0.004*\"condition\" + 0.003*\"detection\"\n",
            "\n",
            "Topic 3: 0.010*\"method\" + 0.007*\"model\" + 0.006*\"state\" + 0.005*\"result\" + 0.004*\"group\" + 0.004*\"data\" + 0.004*\"q\" + 0.004*\"show\" + 0.004*\"cell\" + 0.003*\"quantum\"\n",
            "\n",
            "Topic 4: 0.007*\"model\" + 0.005*\"based\" + 0.005*\"method\" + 0.005*\"network\" + 0.005*\"system\" + 0.005*\"also\" + 0.004*\"measure\" + 0.004*\"problem\" + 0.004*\"sample\" + 0.004*\"proposed\"\n",
            "\n",
            "Topic 1: 24.00%\n",
            "Topic 2: 27.00%\n",
            "Topic 3: 26.00%\n",
            "Topic 4: 23.00%\n",
            "\n",
            "Number of Topics: 6\n",
            "Coherence Score: 0.4054504966124131\n",
            "\n",
            "Topic 1: 0.006*\"approach\" + 0.006*\"model\" + 0.005*\"epidemic\" + 0.004*\"system\" + 0.004*\"show\" + 0.004*\"robot\" + 0.004*\"paper\" + 0.004*\"human\" + 0.004*\"algorithm\" + 0.004*\"performance\"\n",
            "\n",
            "Topic 2: 0.009*\"network\" + 0.007*\"model\" + 0.006*\"system\" + 0.005*\"function\" + 0.005*\"complex\" + 0.005*\"using\" + 0.004*\"flow\" + 0.004*\"detection\" + 0.004*\"data\" + 0.004*\"region\"\n",
            "\n",
            "Topic 3: 0.011*\"method\" + 0.007*\"result\" + 0.006*\"state\" + 0.006*\"model\" + 0.005*\"group\" + 0.005*\"sample\" + 0.005*\"q\" + 0.004*\"show\" + 0.004*\"data\" + 0.004*\"drone\"\n",
            "\n",
            "Topic 4: 0.009*\"model\" + 0.007*\"equation\" + 0.007*\"sample\" + 0.006*\"system\" + 0.006*\"based\" + 0.006*\"adversarial\" + 0.005*\"measure\" + 0.005*\"proposed\" + 0.005*\"network\" + 0.005*\"posterior\"\n",
            "\n",
            "Topic 5: 0.007*\"method\" + 0.006*\"model\" + 0.006*\"learning\" + 0.006*\"representation\" + 0.005*\"state\" + 0.005*\"also\" + 0.005*\"formation\" + 0.005*\"star\" + 0.005*\"application\" + 0.004*\"approach\"\n",
            "\n",
            "Topic 6: 0.012*\"model\" + 0.008*\"sequence\" + 0.005*\"cell\" + 0.004*\"simple\" + 0.004*\"particular\" + 0.004*\"kernel\" + 0.004*\"phylogeny\" + 0.004*\"cnts\" + 0.004*\"vertex\" + 0.004*\"monolayer\"\n",
            "\n",
            "Topic 1: 16.00%\n",
            "Topic 2: 20.00%\n",
            "Topic 3: 21.00%\n",
            "Topic 4: 14.00%\n",
            "Topic 5: 19.00%\n",
            "Topic 6: 10.00%\n",
            "\n",
            "Number of Topics: 8\n",
            "Coherence Score: 0.4038018046834254\n",
            "\n",
            "Topic 1: 0.006*\"system\" + 0.005*\"framework\" + 0.005*\"algorithm\" + 0.005*\"des\" + 0.005*\"detectability\" + 0.005*\"resource\" + 0.005*\"vo\" + 0.004*\"paper\" + 0.004*\"transition\" + 0.004*\"proposed\"\n",
            "\n",
            "Topic 2: 0.011*\"network\" + 0.009*\"model\" + 0.008*\"data\" + 0.006*\"detection\" + 0.006*\"using\" + 0.005*\"domain\" + 0.005*\"complex\" + 0.005*\"flow\" + 0.005*\"source\" + 0.005*\"impedance\"\n",
            "\n",
            "Topic 3: 0.013*\"method\" + 0.008*\"model\" + 0.007*\"result\" + 0.006*\"group\" + 0.006*\"q\" + 0.005*\"epidemic\" + 0.004*\"case\" + 0.004*\"g\" + 0.004*\"chr\" + 0.004*\"data\"\n",
            "\n",
            "Topic 4: 0.009*\"network\" + 0.007*\"system\" + 0.007*\"method\" + 0.006*\"equation\" + 0.006*\"based\" + 0.006*\"performance\" + 0.006*\"model\" + 0.006*\"neural\" + 0.005*\"proposed\" + 0.005*\"also\"\n",
            "\n",
            "Topic 5: 0.011*\"state\" + 0.007*\"approach\" + 0.006*\"representation\" + 0.006*\"x\" + 0.006*\"model\" + 0.006*\"learning\" + 0.005*\"formation\" + 0.005*\"star\" + 0.005*\"show\" + 0.005*\"framework\"\n",
            "\n",
            "Topic 6: 0.015*\"model\" + 0.009*\"sequence\" + 0.008*\"cell\" + 0.007*\"drone\" + 0.004*\"particular\" + 0.004*\"simple\" + 0.004*\"small\" + 0.004*\"kernel\" + 0.004*\"phylogeny\" + 0.004*\"vertex\"\n",
            "\n",
            "Topic 7: 0.006*\"system\" + 0.006*\"posterior\" + 0.006*\"model\" + 0.005*\"field\" + 0.005*\"mfvb\" + 0.005*\"result\" + 0.005*\"fracture\" + 0.004*\"number\" + 0.004*\"measure\" + 0.004*\"one\"\n",
            "\n",
            "Topic 8: 0.016*\"sample\" + 0.010*\"method\" + 0.009*\"model\" + 0.008*\"result\" + 0.008*\"adversarial\" + 0.008*\"nta\" + 0.007*\"space\" + 0.007*\"ev\" + 0.005*\"propose\" + 0.005*\"use\"\n",
            "\n",
            "Topic 1: 11.00%\n",
            "Topic 2: 13.00%\n",
            "Topic 3: 15.00%\n",
            "Topic 4: 13.00%\n",
            "Topic 5: 19.00%\n",
            "Topic 6: 10.00%\n",
            "Topic 7: 14.00%\n",
            "Topic 8: 5.00%\n",
            "\n",
            "Number of Topics: 10\n",
            "Coherence Score: 0.3892700625310904\n",
            "\n",
            "Topic 1: 0.009*\"approach\" + 0.007*\"valley\" + 0.006*\"parameter\" + 0.006*\"proposed\" + 0.006*\"framework\" + 0.006*\"method\" + 0.006*\"model\" + 0.005*\"algorithm\" + 0.005*\"human\" + 0.005*\"shg\"\n",
            "\n",
            "Topic 2: 0.010*\"network\" + 0.008*\"using\" + 0.008*\"model\" + 0.007*\"flow\" + 0.007*\"data\" + 0.007*\"function\" + 0.006*\"system\" + 0.006*\"state\" + 0.005*\"domain\" + 0.005*\"region\"\n",
            "\n",
            "Topic 3: 0.016*\"method\" + 0.009*\"model\" + 0.007*\"group\" + 0.007*\"source\" + 0.006*\"based\" + 0.006*\"epidemic\" + 0.005*\"show\" + 0.005*\"g\" + 0.005*\"chr\" + 0.005*\"sound\"\n",
            "\n",
            "Topic 4: 0.009*\"system\" + 0.009*\"model\" + 0.008*\"equation\" + 0.008*\"based\" + 0.007*\"network\" + 0.007*\"method\" + 0.005*\"proposed\" + 0.005*\"neural\" + 0.005*\"basis\" + 0.005*\"receiver\"\n",
            "\n",
            "Topic 5: 0.010*\"state\" + 0.008*\"representation\" + 0.008*\"x\" + 0.006*\"also\" + 0.006*\"performance\" + 0.006*\"drone\" + 0.005*\"model\" + 0.005*\"show\" + 0.005*\"learning\" + 0.005*\"framework\"\n",
            "\n",
            "Topic 6: 0.019*\"model\" + 0.013*\"sequence\" + 0.009*\"function\" + 0.007*\"problem\" + 0.007*\"cell\" + 0.006*\"analysis\" + 0.006*\"simple\" + 0.006*\"particular\" + 0.006*\"kernel\" + 0.006*\"detection\"\n",
            "\n",
            "Topic 7: 0.008*\"model\" + 0.007*\"posterior\" + 0.007*\"effect\" + 0.007*\"mfvb\" + 0.006*\"fracture\" + 0.005*\"material\" + 0.005*\"phase\" + 0.005*\"using\" + 0.005*\"temperature\" + 0.005*\"correlation\"\n",
            "\n",
            "Topic 8: 0.010*\"nta\" + 0.009*\"space\" + 0.009*\"result\" + 0.009*\"ev\" + 0.007*\"method\" + 0.007*\"sample\" + 0.007*\"particle\" + 0.007*\"size\" + 0.005*\"assumption\" + 0.005*\"least\"\n",
            "\n",
            "Topic 9: 0.013*\"sample\" + 0.011*\"system\" + 0.010*\"q\" + 0.007*\"model\" + 0.006*\"case\" + 0.006*\"adversarial\" + 0.006*\"resource\" + 0.006*\"integration\" + 0.006*\"specie\" + 0.005*\"performance\"\n",
            "\n",
            "Topic 10: 0.009*\"formation\" + 0.007*\"star\" + 0.007*\"spur\" + 0.006*\"impedance\" + 0.005*\"gas\" + 0.004*\"class\" + 0.004*\"study\" + 0.004*\"x\" + 0.004*\"spiral\" + 0.004*\"ihs\"\n",
            "\n",
            "Topic 1: 9.00%\n",
            "Topic 2: 10.00%\n",
            "Topic 3: 13.00%\n",
            "Topic 4: 13.00%\n",
            "Topic 5: 14.00%\n",
            "Topic 6: 7.00%\n",
            "Topic 7: 11.00%\n",
            "Topic 8: 4.00%\n",
            "Topic 9: 7.00%\n",
            "Topic 10: 12.00%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada kode di atas, LDA mengaelompokkan data ke 3 label dan 5 label. Untuk menganalisa hasil maka dicetaklah kata-kata dengan bobot yang paling dominan di sini. Contohnya pada saat LDA mengelompokkan kepada 3 label, topik 1 memiliki bobot:\n",
        "\n",
        "`Topic 1: 0.007*\"model\" + 0.006*\"state\" + 0.006*\"system\" + 0.005*\"based\" + 0.005*\"method\" + 0.004*\"used\" + 0.004*\"result\" + 0.004*\"approach\" + 0.004*\"show\" + 0.004*\"term\"\n",
        "`</br>\n",
        "\n",
        "Dalam representasi topik tersebut, setiap kata memiliki bobot yang menunjukkan seberapa penting kata tersebut dalam konteks topik tertentu. Bobot ini dihitung berdasarkan frekuensi kemunculan kata tersebut dalam korpus dan berapa banyak kata tersebut terkait dengan topik tersebut. model, state, system, based, dll, merupakan kata dengan bobot yang menunjukan bahwa kata-kata ini sangat relevan dengan topik 1.\n",
        "\n",
        "Coherence score didapatkan dengan menggunakan metode c_v. Untuk ukurannya, biasanya menggunakan skala berikut:\n",
        "- Skor di bawah 0,2: Koherensi yang rendah atau tidak relevan.\n",
        "- Skor antara 0,2 hingga 0,4: Koherensi yang cukup baik.\n",
        "- Skor antara 0,4 hingga 0,6: Koherensi yang baik.\n",
        "- Skor di atas 0,6: Koherensi yang sangat baik.\n",
        "\n",
        "Namun kita dapatkan skor berikut:\n",
        "- 2 topik = Coherence Score: 0.290093230451281\n",
        "- 4 topik = Coherence Score: 0.34938396677322825\n",
        "- 6 topik = Coherence Score: 0.4054504966124131\n",
        "- 8 topik = Coherence Score: 0.4038018046834254\n",
        "- 10 topik = Coherence Score: 0.3892700625310904\n",
        "\n",
        "Berarti hasil yang lebih optimal adalah dengan 6 jumlah topik."
      ],
      "metadata": {
        "id": "v5SK3-Jd-gDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Persona Analysis"
      ],
      "metadata": {
        "id": "IF6jHCjoLMcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Persona analysis dapat dilakukan dengan mengidentifikasi dan menganalisis topik yang paling relevan atau signifikan untuk setiap kelompok pengguna atau audiens. Dalam hal ini, topik-topik yang muncul dalam analisis topik dapat digunakan sebagai fitur untuk menggambarkan karakteristik dan preferensi kelompok pengguna atau audiens yang berbeda."
      ],
      "metadata": {
        "id": "BGSerTm7ZE4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train LDA model\n",
        "num_topics = int(input(\"Enter the total number of topics: \"))\n",
        "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=50, random_state = 69)\n",
        "\n",
        "# Print topics\n",
        "topics = lda_model.print_topics(num_topics=num_topics)\n",
        "for topic in topics:\n",
        "    topic_num = topic[0] + 1  # Add 1 to start topic number from 1\n",
        "    print(f\"Topic {topic_num}: {topic[1]}\\n\")\n",
        "\n",
        "# Prompt user to analyze a specific topic\n",
        "topic_to_analyze = int(input(\"Enter the topic number to analyze: \"))\n",
        "\n",
        "# Print abstracts in the selected topic\n",
        "topic_documents = []\n",
        "for i, doc in enumerate(corpus):\n",
        "    topic_distribution = lda_model[doc]\n",
        "    sorted_topics = sorted(topic_distribution, key=lambda x: x[1], reverse=True)\n",
        "    top_topic = sorted_topics[0][0] + 1  # Add 1 to start topic number from 1\n",
        "    if top_topic == topic_to_analyze:\n",
        "        topic_documents.append(data['ABSTRACT'][i])\n",
        "\n",
        "print(f\"\\nDocuments in Topic {topic_to_analyze}:\")\n",
        "for document in topic_documents:\n",
        "    print(\"- \" + document)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVVgH30y5ku9",
        "outputId": "090060f7-e859-453c-b9c8-83812381aa20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the total number of topics: 6\n",
            "Topic 1: 0.006*\"approach\" + 0.006*\"model\" + 0.005*\"epidemic\" + 0.004*\"system\" + 0.004*\"show\" + 0.004*\"robot\" + 0.004*\"paper\" + 0.004*\"human\" + 0.004*\"algorithm\" + 0.004*\"performance\"\n",
            "\n",
            "Topic 2: 0.009*\"network\" + 0.007*\"model\" + 0.006*\"system\" + 0.005*\"function\" + 0.005*\"complex\" + 0.005*\"using\" + 0.004*\"flow\" + 0.004*\"detection\" + 0.004*\"data\" + 0.004*\"region\"\n",
            "\n",
            "Topic 3: 0.011*\"method\" + 0.007*\"result\" + 0.006*\"state\" + 0.006*\"model\" + 0.005*\"group\" + 0.005*\"sample\" + 0.005*\"q\" + 0.004*\"show\" + 0.004*\"data\" + 0.004*\"drone\"\n",
            "\n",
            "Topic 4: 0.009*\"model\" + 0.007*\"equation\" + 0.007*\"sample\" + 0.006*\"system\" + 0.006*\"based\" + 0.006*\"adversarial\" + 0.005*\"measure\" + 0.005*\"proposed\" + 0.005*\"network\" + 0.005*\"posterior\"\n",
            "\n",
            "Topic 5: 0.007*\"method\" + 0.006*\"model\" + 0.006*\"learning\" + 0.006*\"representation\" + 0.005*\"state\" + 0.005*\"also\" + 0.005*\"formation\" + 0.005*\"star\" + 0.005*\"application\" + 0.004*\"approach\"\n",
            "\n",
            "Topic 6: 0.012*\"model\" + 0.008*\"sequence\" + 0.005*\"cell\" + 0.004*\"simple\" + 0.004*\"particular\" + 0.004*\"kernel\" + 0.004*\"phylogeny\" + 0.004*\"cnts\" + 0.004*\"vertex\" + 0.004*\"monolayer\"\n",
            "\n",
            "Enter the topic number to analyze: 3\n",
            "\n",
            "Documents in Topic 3:\n",
            "-   Fourier-transform infra-red (FTIR) spectra of samples from 7 plant species\n",
            "were used to explore the influence of preprocessing and feature extraction on\n",
            "efficiency of machine learning algorithms. Wavelet Tensor Train (WTT) and\n",
            "Discrete Wavelet Transforms (DWT) were compared as feature extraction\n",
            "techniques for FTIR data of medicinal plants. Various combinations of signal\n",
            "processing steps showed different behavior when applied to classification and\n",
            "clustering tasks. Best results for WTT and DWT found through grid search were\n",
            "similar, significantly improving quality of clustering as well as\n",
            "classification accuracy for tuned logistic regression in comparison to original\n",
            "spectra. Unlike DWT, WTT has only one parameter to be tuned (rank), making it a\n",
            "more versatile and easier to use as a data processing tool in various signal\n",
            "processing applications.\n",
            "\n",
            "\n",
            "-   The ability of metallic nanoparticles to supply heat to a liquid environment\n",
            "under exposure to an external optical field has attracted growing interest for\n",
            "biomedical applications. Controlling the thermal transport properties at a\n",
            "solid-liquid interface then appears to be particularly relevant. In this work,\n",
            "we address the thermal transport between water and a gold surface coated by a\n",
            "polymer layer. Using molecular dynamics simulations, we demonstrate that\n",
            "increasing the polymer density displaces the domain resisting to the heat flow,\n",
            "while it doesn't affect the final amount of thermal energy released in the\n",
            "liquid. This unexpected behavior results from a trade-off established by the\n",
            "increasing polymer density which couples more efficiently with the solid but\n",
            "initiates a counterbalancing resistance with the liquid.\n",
            "\n",
            "\n",
            "-   We study the exciton magnetic polaron (EMP) formation in (Cd,Mn)Se/(Cd,Mg)Se\n",
            "diluted-magnetic-semiconductor quantum wells using time-resolved\n",
            "photoluminescence (PL). The magnetic field and temperature dependencies of this\n",
            "dynamics allow us to separate the non-magnetic and magnetic contributions to\n",
            "the exciton localization. We deduce the EMP energy of 14 meV, which is in\n",
            "agreement with time-integrated measurements based on selective excitation and\n",
            "the magnetic field dependence of the PL circular polarization degree. The\n",
            "polaron formation time of 500 ps is significantly longer than the corresponding\n",
            "values reported earlier. We propose that this behavior is related to strong\n",
            "self-localization of the EMP, accompanied with a squeezing of the heavy-hole\n",
            "envelope wavefunction. This conclusion is also supported by the decrease of the\n",
            "exciton lifetime from 600 ps to 200 - 400 ps with increasing magnetic field and\n",
            "temperature.\n",
            "\n",
            "\n",
            "-   We describe a variant construction of the unstable Adams spectral the\n",
            "sequence for a space $Y$, associated to any free simplicial resolution of\n",
            "$H^*(Y;R)$ for $R=\\mathbb{F}_p$ or $\\mathbb{Q}$. We use this construction to\n",
            "describe the differentials and filtration in the spectral sequence in terms of\n",
            "appropriate systems of higher cohomology operations.\n",
            "\n",
            "\n",
            "-   When investigators seek to estimate causal effects, they often assume that\n",
            "selection into treatment is based only on observed covariates. Under this\n",
            "identification strategy, analysts must adjust for observed confounders. While\n",
            "basic regression models have long been the dominant method of statistical\n",
            "adjustment, more robust methods based on matching or weighting have become more\n",
            "common. Of late, even more flexible methods based on machine learning methods\n",
            "have been developed for statistical adjustment. These machine learning methods\n",
            "are designed to be black box methods with little input from the researcher.\n",
            "Recent research used a data competition to evaluate various methods of\n",
            "statistical adjustment and found that black box methods out performed all other\n",
            "methods of statistical adjustment. Matching methods with covariate\n",
            "prioritization are designed for direct input from substantive investigators in\n",
            "direct contrast to black methods. In this article, we use a different research\n",
            "design to compare matching with covariate prioritization to black box methods.\n",
            "We use black box methods to replicate results from five studies where matching\n",
            "with covariate prioritization was used to customize the statistical adjustment\n",
            "in direct response to substantive expertise. We find little difference across\n",
            "the methods. We conclude with advice for investigators.\n",
            "\n",
            "\n",
            "-   Let $X$ be a partially ordered set with the property that each family of\n",
            "order intervals of the form $[a,b],[a,\\rightarrow )$ with the finite\n",
            "intersection property has a nonempty intersection. We show that every directed\n",
            "subset of $X$ has a supremum. Then we apply the above result to prove that if\n",
            "$X$ is a topological space with a partial order $\\preceq $ for which the order\n",
            "intervals are compact, $\\mathcal{F}$ a nonempty commutative family of monotone\n",
            "maps from $X$ into $X$ and there exists $c\\in X$ such that $c\\preceq Tc$ for\n",
            "every $T\\in \\mathcal{F}$, then the set of common fixed points of $\\mathcal{F}$\n",
            "is nonempty and has a maximal element. The result, specialized to the case of\n",
            "Banach spaces gives a general fixed point theorem that drops almost all\n",
            "assumptions from the recent results in this area. An application to the theory\n",
            "of integral equations of Urysohn's type is also given.\n",
            "\n",
            "\n",
            "-   Efficient methods are proposed, for computing integrals appeaing in\n",
            "electronic structure calculations. The methods consist of two parts: the first\n",
            "part is to represent the integrals as contour integrals and the second one is\n",
            "to evaluate the contour integrals by the Clenshaw-Curtis quadrature. The\n",
            "efficiency of the proposed methods is demonstrated through numerical\n",
            "experiments.\n",
            "\n",
            "\n",
            "-   We consider the problem of estimating the $L_1$ distance between two discrete\n",
            "probability measures $P$ and $Q$ from empirical data in a nonasymptotic and\n",
            "large alphabet setting. When $Q$ is known and one obtains $n$ samples from $P$,\n",
            "we show that for every $Q$, the minimax rate-optimal estimator with $n$ samples\n",
            "achieves performance comparable to that of the maximum likelihood estimator\n",
            "(MLE) with $n\\ln n$ samples. When both $P$ and $Q$ are unknown, we construct\n",
            "minimax rate-optimal estimators whose worst case performance is essentially\n",
            "that of the known $Q$ case with $Q$ being uniform, implying that $Q$ being\n",
            "uniform is essentially the most difficult case. The \\emph{effective sample size\n",
            "enlargement} phenomenon, identified in Jiao \\emph{et al.} (2015), holds both in\n",
            "the known $Q$ case for every $Q$ and the $Q$ unknown case. However, the\n",
            "construction of optimal estimators for $\\|P-Q\\|_1$ requires new techniques and\n",
            "insights beyond the approximation-based method of functional estimation in Jiao\n",
            "\\emph{et al.} (2015).\n",
            "\n",
            "\n",
            "-   In 1978 Brakke introduced the mean curvature flow in the setting of geometric\n",
            "measure theory. There exist multiple variants of the original definition. Here\n",
            "we prove that most of them are indeed equal. One central point is to correct\n",
            "the proof of Brakke's §3.5, where he develops an estimate for the evolution\n",
            "of the measure of time-dependent test functions.\n",
            "\n",
            "\n",
            "-   With recent advancements in drone technology, researchers are now considering\n",
            "the possibility of deploying small cells served by base stations mounted on\n",
            "flying drones. A major advantage of such drone small cells is that the\n",
            "operators can quickly provide cellular services in areas of urgent demand\n",
            "without having to pre-install any infrastructure. Since the base station is\n",
            "attached to the drone, technically it is feasible for the base station to\n",
            "dynamic reposition itself in response to the changing locations of users for\n",
            "reducing the communication distance, decreasing the probability of signal\n",
            "blocking, and ultimately increasing the spectral efficiency. In this paper, we\n",
            "first propose distributed algorithms for autonomous control of drone movements,\n",
            "and then model and analyse the spectral efficiency performance of a drone small\n",
            "cell to shed new light on the fundamental benefits of dynamic repositioning. We\n",
            "show that, with dynamic repositioning, the spectral efficiency of drone small\n",
            "cells can be increased by nearly 100\\% for realistic drone speed, height, and\n",
            "user traffic model and without incurring any major increase in drone energy\n",
            "consumption.\n",
            "\n",
            "\n",
            "-   We present an investigation of the supernova remnant (SNR) G306.3$-$0.9 using\n",
            "archival multi-wavelength data. The Suzaku spectra are well described by\n",
            "two-component thermal plasma models: The soft component is in ionization\n",
            "equilibrium and has a temperature $\\sim$0.59 keV, while the hard component has\n",
            "temperature $\\sim$3.2 keV and ionization time-scale $\\sim$$2.6\\times10^{10}$\n",
            "cm$^{-3}$ s. We clearly detected Fe K-shell line at energy of $\\sim$6.5 keV\n",
            "from this remnant. The overabundances of Si, S, Ar, Ca, and Fe confirm that the\n",
            "X-ray emission has an ejecta origin. The centroid energy of the Fe-K line\n",
            "supports that G306.3$-$0.9 is a remnant of a Type Ia supernova (SN) rather than\n",
            "a core-collapse SN. The GeV gamma-ray emission from G306.3$-$0.9 and its\n",
            "surrounding were analyzed using about 6 years of Fermi data. We report about\n",
            "the non-detection of G306.3$-$0.9 and the detection of a new extended gamma-ray\n",
            "source in the south-west of G306.3$-$0.9 with a significance of\n",
            "$\\sim$13$\\sigma$. We discuss several scenarios for these results with the help\n",
            "of data from other wavebands to understand the SNR and its neighborhood.\n",
            "\n",
            "\n",
            "-   Nonclassical states of a quantized light are described in terms of\n",
            "Glauber-Sudarshan P distribution which is not a genuine classical probability\n",
            "distribution. Despite several attempts, defining a uniform measure of\n",
            "nonclassicality (NC) for the single mode quantum states of light is yet an open\n",
            "task. In our previous work [Phys. Rev. A 95, 012330 (2017)] we have shown that\n",
            "the existing well-known measures fail to quantify the NC of single mode states\n",
            "that are generated under multiple NC-inducing operations. Recently, Ivan et.\n",
            "al. [Quantum. Inf. Process. 11, 853 (2012)] have defined a measure of\n",
            "non-Gaussian character of quantum optical states in terms of Wehrl entropy.\n",
            "Here, we adopt this concept in the context of single mode NC. In this paper, we\n",
            "propose a new quantification of NC for the single mode quantum states of light\n",
            "as the difference between the total Wehrl entropy of the state and the maximum\n",
            "Wehrl entropy arising due to its classical characteristics. This we achieve by\n",
            "subtracting from its Wehrl entropy, the maximum Wehrl entropy attainable by any\n",
            "classical state that has same randomness as measured in terms of von-Neumann\n",
            "entropy. We obtain analytic expressions of NC for most of the states, in\n",
            "particular, all pure states and Gaussian mixed states. However, the evaluation\n",
            "of NC for the non-Gaussian mixed states is subject to extensive numerical\n",
            "computation that lies beyond the scope of the current work. We show that, along\n",
            "with the states generated under single NC-inducing operations, also for the\n",
            "broader class of states that are generated under multiple NC-inducing\n",
            "operations, our quantification enumerates the NC consistently.\n",
            "\n",
            "\n",
            "-   Constraint Handling Rules is an effective concurrent declarative programming\n",
            "language and a versatile computational logic formalism. CHR programs consist of\n",
            "guarded reactive rules that transform multisets of constraints. One of the main\n",
            "features of CHR is its inherent concurrency. Intuitively, rules can be applied\n",
            "to parts of a multiset in parallel. In this comprehensive survey, we give an\n",
            "overview of concurrent and parallel as well as distributed CHR semantics,\n",
            "standard and more exotic, that have been proposed over the years at various\n",
            "levels of refinement. These semantics range from the abstract to the concrete.\n",
            "They are related by formal soundness results. Their correctness is established\n",
            "as correspondence between parallel and sequential computations. We present\n",
            "common concise sample CHR programs that have been widely used in experiments\n",
            "and benchmarks. We review parallel CHR implementations in software and\n",
            "hardware. The experimental results obtained show a consistent parallel speedup.\n",
            "Most implementations are available online. The CHR formalism can also be used\n",
            "to implement and reason with models for concurrency. To this end, the Software\n",
            "Transaction Model, the Actor Model, Colored Petri Nets and the Join-Calculus\n",
            "have been faithfully encoded in CHR. Under consideration in Theory and Practice\n",
            "of Logic Programming (TPLP).\n",
            "\n",
            "\n",
            "-   Today's landscape of robotics is dominated by vertical integration where\n",
            "single vendors develop the final product leading to slow progress, expensive\n",
            "products and customer lock-in. Opposite to this, an horizontal integration\n",
            "would result in a rapid development of cost-effective mass-market products with\n",
            "an additional consumer empowerment. The transition of an industry from vertical\n",
            "integration to horizontal integration is typically catalysed by de facto\n",
            "industry standards that enable a simplified and seamless integration of\n",
            "products. However, in robotics there is currently no leading candidate for a\n",
            "global plug-and-play standard.\n",
            "This paper tackles the problem of incompatibility between robot components\n",
            "that hinder the reconfigurability and flexibility demanded by the robotics\n",
            "industry. Particularly, it presents a model to create plug-and-play robot\n",
            "hardware components. Rather than iteratively evolving previous ontologies, our\n",
            "proposed model answers the needs identified by the industry while facilitating\n",
            "interoperability, measurability and comparability of robotics technology. Our\n",
            "approach differs significantly with the ones presented before as it is\n",
            "hardware-oriented and establishes a clear set of actions towards the\n",
            "integration of this model in real environments and with real manufacturers.\n",
            "\n",
            "\n",
            "-   This paper studies the emotion recognition from musical tracks in the\n",
            "2-dimensional valence-arousal (V-A) emotional space. We propose a method based\n",
            "on convolutional (CNN) and recurrent neural networks (RNN), having\n",
            "significantly fewer parameters compared with the state-of-the-art method for\n",
            "the same task. We utilize one CNN layer followed by two branches of RNNs\n",
            "trained separately for arousal and valence. The method was evaluated using the\n",
            "'MediaEval2015 emotion in music' dataset. We achieved an RMSE of 0.202 for\n",
            "arousal and 0.268 for valence, which is the best result reported on this\n",
            "dataset.\n",
            "\n",
            "\n",
            "-   We present muon spin rotation measurements on superconducting Cu intercalated\n",
            "Bi$_2$Se$_3$, which was suggested as a realization of a topological\n",
            "superconductor. We observe a clear evidence of the superconducting transition\n",
            "below 4 K, where the width of magnetic field distribution increases as the\n",
            "temperature is decreased. The measured broadening at mK temperatures suggests a\n",
            "large London penetration depth in the $ab$ plane ($\\lambda_{\\mathrm{eff}}\\sim\n",
            "1.6$ $\\mathrm{\\mu}$m). We show that the temperature dependence of this\n",
            "broadening follows the BCS prediction, but could be consistent with several gap\n",
            "symmetries.\n",
            "\n",
            "\n",
            "-   We report on experimentally measured light shifts of superconducting flux\n",
            "qubits deep-strongly coupled to LC oscillators, where the coupling constants\n",
            "are comparable to the qubit and oscillator resonance frequencies. By using\n",
            "two-tone spectroscopy, the energies of the six lowest levels of each circuit\n",
            "are determined. We find huge Lamb shifts that exceed 90% of the bare qubit\n",
            "frequencies and inversions of the qubits' ground and excited states when there\n",
            "are a finite number of photons in the oscillator. Our experimental results\n",
            "agree with theoretical predictions based on the quantum Rabi model.\n",
            "\n",
            "\n",
            "-   Let $K$ be a function field over a finite field $k$ of characteristic $p$ and\n",
            "let $K_{\\infty}/K$ be a geometric extension with Galois group $\\mathbb{Z}_p$.\n",
            "Let $K_n$ be the corresponding subextension with Galois group\n",
            "$\\mathbb{Z}/p^n\\mathbb{Z}$ and genus $g_n$. In this paper, we give a simple\n",
            "explicit formula $g_n$ in terms of an explicit Witt vector construction of the\n",
            "$\\mathbb{Z}_p$-tower. This formula leads to a tight lower bound on $g_n$ which\n",
            "is quadratic in $p^n$. Furthermore, we determine all $\\mathbb{Z}_p$-towers for\n",
            "which the genus sequence is stable, in the sense that there are $a,b,c \\in\n",
            "\\mathbb{Q}$ such that $g_n=a p^{2n}+b p^n +c$ for $n$ large enough. Such genus\n",
            "stable towers are expected to have strong stable arithmetic properties for\n",
            "their zeta functions. A key technical contribution of this work is a new\n",
            "simplified formula for the Schmid-Witt symbol coming from local class field\n",
            "theory.\n",
            "\n",
            "\n",
            "-   We study the evolution of spin-orbital correlations in an inhomogeneous\n",
            "quantum system with an impurity replacing a doublon by a holon orbital degree\n",
            "of freedom. Spin-orbital entanglement is large when spin correlations are\n",
            "antiferromagnetic, while for a ferromagnetic host we obtain a pure orbital\n",
            "description. In this regime the orbital model can be mapped on spinless\n",
            "fermions and we uncover topological phases with zero energy modes at the edge\n",
            "or at the domain between magnetically inequivalent regions.\n",
            "\n",
            "\n",
            "-   The fundamental group $\\pi$ of a Kodaira fibration is, by definition, the\n",
            "extension of a surface group $\\Pi_b$ by another surface group $\\Pi_g$, i.e. \\[\n",
            "1 \\rightarrow \\Pi_g \\rightarrow \\pi \\rightarrow \\Pi_b \\rightarrow 1. \\]\n",
            "Conversely, we can inquire about what conditions need to be satisfied by a\n",
            "group of that sort in order to be the fundamental group of a Kodaira fibration.\n",
            "In this short note we collect some restriction on the image of the classifying\n",
            "map $m \\colon \\Pi_b \\to \\Gamma_g$ in terms of the coinvariant homology of\n",
            "$\\Pi_g$. In particular, we observe that if $\\pi$ is the fundamental group of a\n",
            "Kodaira fibration with relative irregularity $g-s$, then $g \\leq 1+ 6s$, and we\n",
            "show that this effectively constrains the possible choices for $\\pi$, namely\n",
            "that there are group extensions as above that fail to satisfy this bound, hence\n",
            "cannot be the fundamental group of a Kodaira fibration. In particular this\n",
            "provides examples of symplectic $4$--manifolds that fail to admit a Kähler\n",
            "structure for reasons that eschew the usual obstructions.\n",
            "\n",
            "\n",
            "-   The interest in the extracellular vesicles (EVs) is rapidly growing as they\n",
            "became reliable biomarkers for many diseases. For this reason, fast and\n",
            "accurate techniques of EVs size characterization are the matter of utmost\n",
            "importance. One increasingly popular technique is the Nanoparticle Tracking\n",
            "Analysis (NTA), in which the diameters of EVs are calculated from their\n",
            "diffusion constants. The crucial assumption here is that the diffusion in NTA\n",
            "follows the Stokes-Einstein relation, i.e. that the Mean Square Displacement\n",
            "(MSD) of a particle grows linearly in time (MSD $\\propto t$). However, we show\n",
            "that NTA violates this assumption in both artificial and biological samples,\n",
            "i.e. a large population of particles show a strongly sub-diffusive behaviour\n",
            "(MSD $\\propto t^\\alpha$, $0<\\alpha<1$). To support this observation we present\n",
            "a range of experimental results for both polystyrene beads and EVs. This is\n",
            "also related to another problem: for the same samples there exists a huge\n",
            "discrepancy (by the factor of 2-4) between the sizes measured with NTA and with\n",
            "the direct imaging methods, such as AFM. This can be remedied by e.g. the\n",
            "Finite Track Length Adjustment (FTLA) method in NTA, but its applicability is\n",
            "limited in the biological and poly-disperse samples. On the other hand, the\n",
            "models of sub-diffusion rarely provide the direct relation between the size of\n",
            "a particle and the generalized diffusion constant. However, we solve this last\n",
            "problem by introducing the logarithmic model of sub-diffusion, aimed at\n",
            "retrieving the size data. In result, we propose a novel protocol of NTA data\n",
            "analysis. The accuracy of our method is on par with FTLA for small\n",
            "($\\simeq$200nm) particles. We apply our method to study the EVs samples and\n",
            "corroborate the results with AFM.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output di atas didapatkan dengan cara memilih topik 3 dari total 6 total topik. Setelah proses pemeriksaan dapat diperkirakan bahwa topik 3 ini mencakup topik pada ranah matematika. Namun patut diakui bahwa pengelompokan persona analysis cukup sulit dilakukan tanpa pengetahuan mendalam dari ranah paper tersebut.\n",
        "Maka dengan melakukan persona analysis untuk abstrak yang terkelompokkan pada topik 2, kelompok ini dapat teridentifikasi secara manual dan ditujukan kepada pihak atau lembaga yang mendalami matematika. Bahkan untuk tahap selanjutnya hasil ini dapat digunakan untuk proses labelling, sehingga data ini dapat dipakai untuk membuat model prediksi supervised.\n",
        "\n",
        "Namun, menurut saya pribadi hasilnya kurang valid, ada banyak dokumen yang menurut saya seharusnya tidak termasuk dalam topik sama, seperti biologi dengan deep learning. Mungkin perlu dilakukan analisis lebih lanjut lagi untuk hasil elbih bagus. Saya berasumsi kalau hal hal ini terjadi karena kekurangan data yang hanya berjumlah 100 row. Untuk pengelompokkan lebih baik patut dilaksanakan percobaan baru dengan data yang jauh lebih banyak."
      ],
      "metadata": {
        "id": "6ClRz00wOo-o"
      }
    }
  ]
}